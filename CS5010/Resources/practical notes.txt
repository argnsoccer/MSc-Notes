References for practical:   
    Leibniz argument against mechanism (in Monadology axiom 17) - 1714 (only in context of the Chinese Room)
    Turing - Mind/Turing Test - 1950
        “The Extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training 
        as by the properties of the object under consideration. If we are able to explain and predict its behaviour or if there seems to be 
        little underlying plan, we have little temptation to imagine intelligence. With the same object, therefore, it is possible that one man 
        would consider it as intelligent and another would not, the second man would have found out the rules of its behaviour”.
                           
    Alan Newell & Herbert Simon - 1955 - concluded strings of bits could stand for anything, even features of real world
        Physical Symbol System Hypothesis - A physical symbol system has the necessary and sufficient means for general intelligent Association
        By "necessary" we mean that any system that exhibits general intelligence will prove upon analysis to be a  physical symbol system. 
        By "sufficient" we mean that any physical symbol system of sufficient size can be organised to exhibit general intelligence.
        **** Intellectual issues in the history of artificial intelligence A Newell 1983 ***
            Algorithmic vs Heuristic
            "For a short while, one name for the field of AI was heuristic programming, reflecting, in part, a coordination with such subfields 
            as linear programming and dynamic programming".
            Search versus Expertise
            heuristic -> knowledge-intensive programs (expert systems) (mid 1970s)
            heuristic search (search of problem space - combinatorial) & 
            search of system's memory for knowledge to guide the heuristic search 
                (essentially a knowledge base - made to be accessed quickly (HashTable whatever)).

           

                


                    


        Herb Simon [Simon 1984] defines learning as ‘... any change in a system that allows it to perform better 
        the second time on repetition of the same task or on another task drawn from the same population'
                            
    Rosenblatt - Perceptron (Linear Binary Classifier) - Neural Nets - 1958/1961 - can "classify" and "learn" versus mere instruction
        Neural Nets - Not given rules like chinese room - legitimately learning - big jump in AI
        PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS 1962
        Michie's MENACE (tic-tac-toe machine using machine learning) - 1980s


     1965-1975 (From 1983 A Newell)
                Need for generality in AI, not power (could not do common sense, but were conquering well-defined difficult tasks) [McCarthy 1959]
                mid 1960s goal shift towards generality and common sense -> using small constructred puzzles and problems to simulate everyday reasoning
                    monkey-and-bananas task
                1975 - expert systems took over a bit but it did not fully shift back, just "diminished and moved out of the limelight." 
                    chess programs built on heuristic search principles, with emphasis on large amounts of search...
                    alternative emphases never shut offices just diminish

    1972 - terry winograd - Understanding Natural Language
    combines heuristic search with knowledge base/Expertise
        "The system contains a parser, a recognition grammar of English, programs for semantic analysis, 
        and a general problem solving system. We assume that a computer cannot deal reasonably with language 
        unless it can understand the subject it is discussing. Therefore, the program is given a detailed model 
        of a particular domain. In addition, the system has a simple model of its own mentality. It can remember 
        and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person,
        responding to English sentences with actions and English replies, asking for clarification when its heuristic
        programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. 
        Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. 
        By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. 
        Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system."

    Searle - Chinese Room - 1980 
        Lawrence Davis - telephone lines and offices - 1974 
        Ned Block - China Brain - 1978

    One AI or Many? - 1988
        Another example of why the evolution was slow leading up to the 2000s, because everyone was split on which was right
        AI inherently contains neuroscience, philosophy, etc.
        
        learning vs symbolic systems
        Marvin Minsky and Seymour Papert, Perceptrons: An Introduction to Computational Geometry(1969)
            We do not know the true theory behind Perceptrons   
                but Perceptrons help us do a lot now with neural nets, moving forward progress 
                even without really having a mathematical logical system/theory behind it

                "The current status of deep learning systems can be compared to the age of  steam  that  marked  
                the  beginning  of  the  industrial  age.  The  steam engine  began  transforming  the  world
                long  before  the  formulation  of the laws of thermodynamics. The massive technological advances that 
                have shaped our modern world, however, would have been unthinkable without  thermodynamics.  
                In  that  sense,  Minsky  and  Papert’s  message remains very relevant."

  



    Penrose - emperor's new mind - 1989 - human consciousness is non-algorithmic
    Penrose - Shadows of the mind - 1994

    Machine Learning - so easy to do - tensorflow/keras/python making it accessible to anyone
        Where is the jump from learning to "understanding?"

    As we learn more about the brain as a bioelectric &chemical machine, we also progress AI

    FUTURE/NOw - not only learning from empty (machine learning) but starting with an initial knowledge base to draw upon (not having to relearn) and be able to store
                            
    http://unesdoc.unesco.org/images/0026/002615/261563E.pdf

        1 - Existing knowledge can be utilized as “Background Knowledge”. This has benefits for the learner - in that it is 
            not forced to relearn every thing ever time.
        2 - New Learned Knowledge can be independently verified. In Turing’s world, it is possible to ask the learner after the 
            fact, “What it is that you have learned?”. The learner can then answer, for example, by emitting its learned model in 
            logic. Even if the model is not perfect, it is in a form that can be tested, debugged, and then adopted into the background 
            knowledge for future learners to use. 

            [King et al 1996].
                "A new structural alert using atoms and their bond connectivities in chemicals was able to predict mutagenicity
                - and the new alert - even though generated by an AI, was published in a peer reviewed science journal.
                Similar techniques were subsequently applied to the discovery of what makes certain chemicals behave as drugs,
                in identifying the active subcomponent of the chemicals - from a large number of reactive chemicals. Having identified 
                such a subcomponent (known as a pharmacaphore) it can be synthesised into a safe carrier molecule and be a precursor to 
                a new drug treatment. In this case, the AI tool used was capable of following Turing’s preferred route - utilizing 
                existing chemistry text- book knowledge as background knowledge, a logical representation used to input the example 
                chemicals, and a discovery emitted in a logical output form which could be understood and vetted by a chemist."
                from unesdoc source above -- talking about King et al 1996 study on rats: 
         
                "This was a strong example of AI being involved in a scientific discovery involving humans as part of the process"

            [King et al 2004]
                “Closed Loop Learning" - Nature magazine - big step forward in discovering gene expression
                autonomous AI both hypothesizing and performing experiment
                Here we have AI making scientific discoveries, in a novel way that extends our understanding of scientific method 
                (see [Gillies 1996]).

            "In her presentation at the 2016 NetExplo conference Noriko Aria demonstrated her robot (or AI software) that had 
            successfully passed the Japanese university entrance examinations in History and Mathematics. Her system Todai 
            was able to prove end of high-school level mathematical theorems [Wada et al 2015]. This was quite an achievement 
            for a machine. However, future developments might make it more challenging for human mathematicians as it is one 
            thing to make a proof, and another to understand it"

            "Many Machine Learning systems do not allow the inspection of their ‘up- dated basis’ by generating opaque (to 
            human) models. Michie characterises his machine learning definition above as that of ‘Weak Machine Learning’ and 
            goes on to define Strong Machine Learning as a system that improves its performance and can ‘communicate its internal updates in explicit symbolic form’."
            "Strong Machine Learning is, however, not what is being used for the bulk of the today’s wave of AI
            these are typically building complex models using deep networks of opaque layers of artificial neurons
            and vast numbers of training examples"

            AGAIN Where is the jump from learning to "understanding?"

    1991 - Python
    2015 - keras (opensource neural net library) and tensorflow (opensource library for dataflow (symbolic math library - hearkening back))